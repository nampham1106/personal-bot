{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d9f3d818-ccee-4a2c-a3e5-e5a5d7f816ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64c79e10-0ab6-4bdf-a046-157bd29f0288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7a2dfd1-3f52-44c3-b7cd-2bed23c93956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv('DATA_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd93c33a-9c2c-4af9-81f5-3114a820d3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Alibaba-NLP/gte-multilingual-base'\n",
    "cache_dir=os.path.join(os.getenv('DATA_PATH'), model_name)\n",
    "model = AutoModel.from_pretrained(model_name, cache_dir=cache_dir, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f3a98ed3-a889-4613-85a5-dffdfd9a78d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "embed_model = SentenceTransformer(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04fc35f2-1dfa-4e35-95e6-6132ed18a5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(NewModel(\n",
       "   (embeddings): NewEmbeddings(\n",
       "     (word_embeddings): Embedding(250048, 768, padding_idx=1)\n",
       "     (rotary_emb): NTKScalingRotaryEmbedding()\n",
       "     (token_type_embeddings): Embedding(1, 768)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (encoder): NewEncoder(\n",
       "     (layer): ModuleList(\n",
       "       (0-11): 12 x NewLayer(\n",
       "         (attention): NewSdpaAttention(\n",
       "           (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "           (dropout): Dropout(p=0.0, inplace=False)\n",
       "           (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "         )\n",
       "         (mlp): NewGatedMLP(\n",
       "           (up_gate_proj): Linear(in_features=768, out_features=6144, bias=False)\n",
       "           (down_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           (act_fn): GELUActivation()\n",
       "           (hidden_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (attn_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "         (mlp_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "         (hidden_dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " XLMRobertaTokenizerFast(name_or_path='Alibaba-NLP/gte-multilingual-base', vocab_size=250002, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       " \t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t250001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       " })"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43262449-fc92-4e41-96a4-d73420aa0c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test = \"Berlin is the capital and largest city of Germany, both by area and by population. Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits. The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a8b30ce-fd40-4933-9e44-fd012845d831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     0,  10271,     83,     70,  10323,    136, 142105,  26349,    111,\n",
       "         102126,      4,  15044,    390,  16128,    136,    390,  43904,      5,\n",
       "           1650,      7,   1286,   3501,   1031,  12951,  19879,     23, 109261,\n",
       "          16037,   3249,    442,     70,  28811,  32528,     25,      7,   2684,\n",
       "         132573,    223,  26349,      4,    237,  72350,     71,    390,  43904,\n",
       "          28032,  26349,  17475,      7,      5,    581,  26349,     83,   2843,\n",
       "           1632,    111,     70, 117249,    111, 102126,      4,    136,     83,\n",
       "             70,  50960,  19336,    525,  11341,     23,     70,  23295,     23,\n",
       "          69407,    111,  16128,      5,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1]]), 'offset_mapping': tensor([[[  0,   0],\n",
       "         [  0,   6],\n",
       "         [  7,   9],\n",
       "         [ 10,  13],\n",
       "         [ 14,  21],\n",
       "         [ 22,  25],\n",
       "         [ 26,  33],\n",
       "         [ 34,  38],\n",
       "         [ 39,  41],\n",
       "         [ 42,  49],\n",
       "         [ 49,  50],\n",
       "         [ 51,  55],\n",
       "         [ 56,  58],\n",
       "         [ 59,  63],\n",
       "         [ 64,  67],\n",
       "         [ 68,  70],\n",
       "         [ 71,  81],\n",
       "         [ 81,  82],\n",
       "         [ 83,  85],\n",
       "         [ 85,  86],\n",
       "         [ 87,  91],\n",
       "         [ 92,  96],\n",
       "         [ 97,  99],\n",
       "         [ 99, 101],\n",
       "         [102, 109],\n",
       "         [110, 112],\n",
       "         [112, 118],\n",
       "         [118, 121],\n",
       "         [122, 126],\n",
       "         [127, 129],\n",
       "         [130, 133],\n",
       "         [134, 142],\n",
       "         [143, 148],\n",
       "         [148, 149],\n",
       "         [149, 150],\n",
       "         [151, 155],\n",
       "         [156, 162],\n",
       "         [162, 164],\n",
       "         [165, 169],\n",
       "         [169, 170],\n",
       "         [171, 173],\n",
       "         [174, 181],\n",
       "         [181, 182],\n",
       "         [183, 185],\n",
       "         [186, 196],\n",
       "         [197, 203],\n",
       "         [204, 208],\n",
       "         [209, 214],\n",
       "         [214, 215],\n",
       "         [215, 216],\n",
       "         [217, 220],\n",
       "         [221, 225],\n",
       "         [226, 228],\n",
       "         [229, 233],\n",
       "         [234, 237],\n",
       "         [238, 240],\n",
       "         [241, 244],\n",
       "         [245, 251],\n",
       "         [252, 254],\n",
       "         [255, 262],\n",
       "         [262, 263],\n",
       "         [264, 267],\n",
       "         [268, 270],\n",
       "         [271, 274],\n",
       "         [275, 280],\n",
       "         [281, 286],\n",
       "         [286, 289],\n",
       "         [290, 295],\n",
       "         [296, 298],\n",
       "         [299, 302],\n",
       "         [303, 310],\n",
       "         [311, 313],\n",
       "         [314, 319],\n",
       "         [320, 322],\n",
       "         [323, 327],\n",
       "         [327, 328],\n",
       "         [  0,   0]]])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(input_test, return_tensors='pt', return_offsets_mapping=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c985ecf-f704-42d3-990c-8a9256212b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation_mark_id = tokenizer.convert_tokens_to_ids('.')\n",
    "sep_id = tokenizer.convert_tokens_to_ids('</s>')\n",
    "punctuation_mark_id, sep_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2bcfb7da-d86f-49dd-bede-1f21f8afab06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  0,   0],\n",
       "         [  0,   6],\n",
       "         [  7,   9],\n",
       "         [ 10,  13],\n",
       "         [ 14,  21],\n",
       "         [ 22,  25],\n",
       "         [ 26,  33],\n",
       "         [ 34,  38],\n",
       "         [ 39,  41],\n",
       "         [ 42,  49],\n",
       "         [ 49,  50],\n",
       "         [ 51,  55],\n",
       "         [ 56,  58],\n",
       "         [ 59,  63],\n",
       "         [ 64,  67],\n",
       "         [ 68,  70],\n",
       "         [ 71,  81],\n",
       "         [ 81,  82],\n",
       "         [ 83,  85],\n",
       "         [ 85,  86],\n",
       "         [ 87,  91],\n",
       "         [ 92,  96],\n",
       "         [ 97,  99],\n",
       "         [ 99, 101],\n",
       "         [102, 109],\n",
       "         [110, 112],\n",
       "         [112, 118],\n",
       "         [118, 121],\n",
       "         [122, 126],\n",
       "         [127, 129],\n",
       "         [130, 133],\n",
       "         [134, 142],\n",
       "         [143, 148],\n",
       "         [148, 149],\n",
       "         [149, 150],\n",
       "         [151, 155],\n",
       "         [156, 162],\n",
       "         [162, 164],\n",
       "         [165, 169],\n",
       "         [169, 170],\n",
       "         [171, 173],\n",
       "         [174, 181],\n",
       "         [181, 182],\n",
       "         [183, 185],\n",
       "         [186, 196],\n",
       "         [197, 203],\n",
       "         [204, 208],\n",
       "         [209, 214],\n",
       "         [214, 215],\n",
       "         [215, 216],\n",
       "         [217, 220],\n",
       "         [221, 225],\n",
       "         [226, 228],\n",
       "         [229, 233],\n",
       "         [234, 237],\n",
       "         [238, 240],\n",
       "         [241, 244],\n",
       "         [245, 251],\n",
       "         [252, 254],\n",
       "         [255, 262],\n",
       "         [262, 263],\n",
       "         [264, 267],\n",
       "         [268, 270],\n",
       "         [271, 274],\n",
       "         [275, 280],\n",
       "         [281, 286],\n",
       "         [286, 289],\n",
       "         [290, 295],\n",
       "         [296, 298],\n",
       "         [299, 302],\n",
       "         [303, 310],\n",
       "         [311, 313],\n",
       "         [314, 319],\n",
       "         [320, 322],\n",
       "         [323, 327],\n",
       "         [327, 328],\n",
       "         [  0,   0]]),\n",
       " tensor([     0,  10271,     83,     70,  10323,    136, 142105,  26349,    111,\n",
       "         102126,      4,  15044,    390,  16128,    136,    390,  43904,      5,\n",
       "           1650,      7,   1286,   3501,   1031,  12951,  19879,     23, 109261,\n",
       "          16037,   3249,    442,     70,  28811,  32528,     25,      7,   2684,\n",
       "         132573,    223,  26349,      4,    237,  72350,     71,    390,  43904,\n",
       "          28032,  26349,  17475,      7,      5,    581,  26349,     83,   2843,\n",
       "           1632,    111,     70, 117249,    111, 102126,      4,    136,     83,\n",
       "             70,  50960,  19336,    525,  11341,     23,     70,  23295,     23,\n",
       "          69407,    111,  16128,      5,      2]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_offsets = inputs['offset_mapping'][0]\n",
    "token_ids = inputs['input_ids'][0]\n",
    "token_offsets, token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a46f53d-aac9-48e2-a2f3-89bb00519972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(17, 82), (49, 216), (75, 328)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_positions = [\n",
    "    (i, int(start + 1))\n",
    "    for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets))\n",
    "    if token_id == punctuation_mark_id\n",
    "    and (\n",
    "        token_offsets[i + 1][0] - token_offsets[i][1] > 0\n",
    "        or token_ids[i + 1] == sep_id\n",
    "    )\n",
    "]\n",
    "chunk_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e88b6d2-26fe-4cab-b809-4afd48b65d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(1, 0), (17, 82), (49, 216)],\n",
       " 'Berlin is the capital and largest city of Germany, both by area and by population.')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(1, 0)] + chunk_positions[:-1], input_test[0: 82]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6924e210-1e8d-42c8-a34d-e0de77b4481d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Berlin is the capital and largest city of Germany, both by area and by population.',\n",
       " \" Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\",\n",
       " ' The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = [\n",
    "    input_test[x[1] : y[1]]\n",
    "    for x, y in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "]\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ae8a5c2-b18e-44fa-b546-5afe74795c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(1, 17), (17, 49), (49, 75)],)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_annotations = [\n",
    "   (x[0], y[0]) for (x, y) in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "],\n",
    "span_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "575c4c93-dadc-4112-83a7-f1db5439e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sentences(input_text: str, tokenizer: callable):\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', return_offsets_mapping=True)\n",
    "    punctuation_mark_id = tokenizer.convert_tokens_to_ids('.')\n",
    "    sep_id = tokenizer.convert_tokens_to_ids('</s>')\n",
    "    token_offsets = inputs['offset_mapping'][0]\n",
    "    token_ids = inputs['input_ids'][0]\n",
    "    chunk_positions = [\n",
    "        (i, int(start + 1))\n",
    "        for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets))\n",
    "        if token_id == punctuation_mark_id\n",
    "        and (\n",
    "            token_offsets[i + 1][0] - token_offsets[i][1] > 0\n",
    "            or token_ids[i + 1] == sep_id\n",
    "        )\n",
    "    ]\n",
    "    chunks = [\n",
    "        input_text[x[1] : y[1]]\n",
    "        for x, y in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    span_annotations = [\n",
    "        (x[0], y[0]) for (x, y) in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    return chunks, span_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cbb32825-dde2-400e-87f3-cdc854ebfc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks: \n",
      "- \"Berlin is the capital and largest city of Germany, both by area and by population.\"\n",
      "-\" Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\"\n",
      "-\" The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"\n",
      "[(1, 17), (17, 49), (49, 75)]\n"
     ]
    }
   ],
   "source": [
    "chunks, span_annotations = chunk_by_sentences(input_test, tokenizer)\n",
    "print('chunks: \\n- \"' + '\"\\n-\"'.join(chunks) + '\"')\n",
    "print(span_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f8fbb8b6-e1f5-4c2f-98b4-41da509af84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def late_chunking(\n",
    "    model_output, span_annotation: list, max_length=None\n",
    "):\n",
    "    token_embeddings = model_output[0]\n",
    "    outputs = []\n",
    "    for embeddings, annotations in zip(token_embeddings, span_annotation):\n",
    "        if (\n",
    "            max_length is not None  \n",
    "        ):\n",
    "            annotations = [\n",
    "                (start, min(end, max_length - 1))\n",
    "                for (start, end) in annotations\n",
    "                if (start < max_length -1)\n",
    "            ]\n",
    "        pooled_embeddings = [\n",
    "            embeddings[start: end].sum(dim=0) / (end - start)\n",
    "            for start, end in annotations\n",
    "            if (end - start) >= 1\n",
    "        ]\n",
    "        pooled_embeddings = [\n",
    "            embedding.detach().cpu().numpy() for embedding in pooled_embeddings\n",
    "        ]\n",
    "        outputs.append(pooled_embeddings)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea071637-ce75-4722-9e35-a3c79c68b1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPooling(last_hidden_state=tensor([[[-3.0373,  2.1600, -1.3345,  ...,  0.2574, -0.6234, -0.9165],\n",
      "         [-1.8130,  1.3040, -0.9710,  ...,  0.5940, -0.1316, -0.3924],\n",
      "         [-1.7915,  1.2069, -0.8465,  ...,  0.5444,  0.1945, -0.3223],\n",
      "         ...,\n",
      "         [-1.8188,  1.0727, -0.9326,  ...,  0.2291,  0.4417, -0.3294],\n",
      "         [-1.8492,  1.1341, -0.9045,  ...,  0.3692, -0.0985, -0.4694],\n",
      "         [-2.2777,  1.6846, -1.0123,  ...,  0.5971, -0.1518, -0.5017]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=None, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "embeddings_traditional_chunking = embed_model.encode(chunks)\n",
    "\n",
    "inputs = tokenizer(input_test, return_tensors='pt')\n",
    "model_output = model(**inputs)\n",
    "print(model_output)\n",
    "embeddings = late_chunking(model_output, [span_annotations])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3ee5d2bf-6b3d-405b-b040-e2caede5148a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity_new(\"Berlin\", \"Berlin is the capital and largest city of Germany, both by area and by population.\"): 0.59084535\n",
      "similarity_trad(\"Berlin\", \"Berlin is the capital and largest city of Germany, both by area and by population.\"): 0.81948066\n",
      "similarity_new(\"Berlin\", \" Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\"): 0.579522\n",
      "similarity_trad(\"Berlin\", \" Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\"): 0.7625047\n",
      "similarity_new(\"Berlin\", \" The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"): 0.5828538\n",
      "similarity_trad(\"Berlin\", \" The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"): 0.76740646\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cos_sim = lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "berlin_embedding = embed_model.encode('Berlin')\n",
    "\n",
    "for chunk, new_embedding, trad_embedding in zip(chunks, embeddings, embeddings_traditional_chunking):\n",
    "    print(f'similarity_new(\"Berlin\", \"{chunk}\"):', cos_sim(berlin_embedding, new_embedding))\n",
    "    print(f'similarity_trad(\"Berlin\", \"{chunk}\"):', cos_sim(berlin_embedding, trad_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783c797c-1589-4f67-abe3-f931262b4975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
